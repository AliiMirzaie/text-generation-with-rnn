{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# Text generation with an RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bGsCP9DZFQ5"
      },
      "source": [
        "While some of the sentences are grammatical, most do not make sense. The model has not learned the meaning of words, but consider:\n",
        "\n",
        "* The model is character-based. When training started, the model did not know how to spell an English word, or that words were even a unit of text.\n",
        "\n",
        "* The structure of the output resembles a play—blocks of text generally begin with a speaker name, in all capital letters similar to the dataset.\n",
        "\n",
        "* As demonstrated below, the model is trained on small batches of text (100 characters each), and is still able to generate a longer sequence of text with coherent structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srXC6pLGLwS6"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:42.483846Z",
          "iopub.status.busy": "2023-03-08T13:42:42.483336Z",
          "iopub.status.idle": "2023-03-08T13:42:44.598483Z",
          "shell.execute_reply": "2023-03-08T13:42:44.597775Z"
        },
        "id": "yG_n40gFzf9s"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = './niz.txt'"
      ],
      "metadata": {
        "id": "euhnHCZnywN_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdCjDuSvX_"
      },
      "source": [
        "### Read the data\n",
        "\n",
        "First, look in the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:44.661302Z",
          "iopub.status.busy": "2023-03-08T13:42:44.660856Z",
          "iopub.status.idle": "2023-03-08T13:42:44.666161Z",
          "shell.execute_reply": "2023-03-08T13:42:44.665599Z"
        },
        "id": "aavnuByVymwK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8e1e622-a9c8-485f-bc2e-b6b5f3a56959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 521681 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path, 'rb').read().decode(encoding='unicode_escape')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:44.668934Z",
          "iopub.status.busy": "2023-03-08T13:42:44.668508Z",
          "iopub.status.idle": "2023-03-08T13:42:44.671865Z",
          "shell.execute_reply": "2023-03-08T13:42:44.671334Z"
        },
        "id": "Duhg9NrUymwO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f66078f5-dd84-4c23-856e-256e1b1cbf6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                  Zarathustra's Prologue\n",
            "\n",
            "                            1.\n",
            "\n",
            "  WHEN Zarathustra was thirty years old, he left his home and the lake\n",
            "of his home, and went into the mountains. There he enjoyed his\n",
            "spirit and his solitude, and for ten year\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:44.674876Z",
          "iopub.status.busy": "2023-03-08T13:42:44.674381Z",
          "iopub.status.idle": "2023-03-08T13:42:44.687951Z",
          "shell.execute_reply": "2023-03-08T13:42:44.687401Z"
        },
        "id": "IlCgQBRVymwR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e193419a-bd43-4145-ccef-4751a643b86f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "74 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNnrKn_lL-IJ"
      },
      "source": [
        "## Process the text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFjSVAlWzf-N"
      },
      "source": [
        "### Vectorize the text\n",
        "\n",
        "Before training, you need to convert the strings to a numerical representation.\n",
        "\n",
        "The `tf.keras.layers.StringLookup` layer can convert each character into a numeric ID. It just needs the text to be split into tokens first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:44.691119Z",
          "iopub.status.busy": "2023-03-08T13:42:44.690697Z",
          "iopub.status.idle": "2023-03-08T13:42:48.425491Z",
          "shell.execute_reply": "2023-03-08T13:42:48.424655Z"
        },
        "id": "a86OoYtO01go",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd9a2c13-923b-4eaa-9b4e-114d0274218f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1s4f1q3iqY8f"
      },
      "source": [
        "Now create the `tf.keras.layers.StringLookup` layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:48.429280Z",
          "iopub.status.busy": "2023-03-08T13:42:48.428548Z",
          "iopub.status.idle": "2023-03-08T13:42:48.444872Z",
          "shell.execute_reply": "2023-03-08T13:42:48.444063Z"
        },
        "id": "6GMlCe3qzaL9"
      },
      "outputs": [],
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZmX_jbgQqfOi"
      },
      "source": [
        "It converts from tokens to character IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:48.447759Z",
          "iopub.status.busy": "2023-03-08T13:42:48.447526Z",
          "iopub.status.idle": "2023-03-08T13:42:48.455899Z",
          "shell.execute_reply": "2023-03-08T13:42:48.455137Z"
        },
        "id": "WLv5Q_2TC2pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea4c3784-7286-4e4a-d135-39ea22982eee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[49, 50, 51, 52, 53, 54, 55], [72, 73, 74]]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZfqhkYCymwX"
      },
      "source": [
        "Since the goal of this tutorial is to generate text, it will also be important to invert this representation and recover human-readable strings from it. For this you can use `tf.keras.layers.StringLookup(..., invert=True)`.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uenivzwqsDhp"
      },
      "source": [
        "Note: Here instead of passing the original vocabulary generated with `sorted(set(text))` use the `get_vocabulary()` method of the `tf.keras.layers.StringLookup` layer so that the `[UNK]` tokens is set the same way."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:48.459153Z",
          "iopub.status.busy": "2023-03-08T13:42:48.458700Z",
          "iopub.status.idle": "2023-03-08T13:42:48.470614Z",
          "shell.execute_reply": "2023-03-08T13:42:48.469743Z"
        },
        "id": "Wd2m3mqkDjRj"
      },
      "outputs": [],
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqTDDxS-s-H8"
      },
      "source": [
        "This layer recovers the characters from the vectors of IDs, and returns them as a `tf.RaggedTensor` of characters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:48.474217Z",
          "iopub.status.busy": "2023-03-08T13:42:48.473465Z",
          "iopub.status.idle": "2023-03-08T13:42:48.479938Z",
          "shell.execute_reply": "2023-03-08T13:42:48.479180Z"
        },
        "id": "c2GCh0ySD44s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2263af4a-9bb8-422b-ac36-b777afd8c994"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FeW5gqutT3o"
      },
      "source": [
        "You can `tf.strings.reduce_join` to join the characters back into strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:48.483120Z",
          "iopub.status.busy": "2023-03-08T13:42:48.482537Z",
          "iopub.status.idle": "2023-03-08T13:42:48.528676Z",
          "shell.execute_reply": "2023-03-08T13:42:48.527923Z"
        },
        "id": "zxYI-PeltqKP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e404cf6d-20f8-4158-9392-f2915667892f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:48.531793Z",
          "iopub.status.busy": "2023-03-08T13:42:48.531197Z",
          "iopub.status.idle": "2023-03-08T13:42:48.534947Z",
          "shell.execute_reply": "2023-03-08T13:42:48.534163Z"
        },
        "id": "w5apvBDn9Ind"
      },
      "outputs": [],
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbmsf23Bymwe"
      },
      "source": [
        "### The prediction task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wssHQ1oGymwe"
      },
      "source": [
        "Given a character, or a sequence of characters, what is the most probable next character? This is the task you're training the model to perform. The input to the model will be a sequence of characters, and you train the model to predict the output—the following character at each time step.\n",
        "\n",
        "Since RNNs maintain an internal state that depends on the previously seen elements, given all the characters computed until this moment, what is the next character?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:48.538366Z",
          "iopub.status.busy": "2023-03-08T13:42:48.537775Z",
          "iopub.status.idle": "2023-03-08T13:42:48.870919Z",
          "shell.execute_reply": "2023-03-08T13:42:48.869864Z"
        },
        "id": "UopbsKi88tm5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fb4a1bc-e81d-494c-e5c7-7992e7c1ea7a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(521681,), dtype=int64, numpy=array([1, 2, 2, ..., 1, 1, 1])>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:48.874389Z",
          "iopub.status.busy": "2023-03-08T13:42:48.874091Z",
          "iopub.status.idle": "2023-03-08T13:42:48.879255Z",
          "shell.execute_reply": "2023-03-08T13:42:48.878480Z"
        },
        "id": "qmxrYDCTy-eL"
      },
      "outputs": [],
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:48.882499Z",
          "iopub.status.busy": "2023-03-08T13:42:48.881871Z",
          "iopub.status.idle": "2023-03-08T13:42:48.914365Z",
          "shell.execute_reply": "2023-03-08T13:42:48.913488Z"
        },
        "id": "cjH5v45-yqqH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ddae91a-45be-4fc1-d439-600a5393fa91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n",
            " \n"
          ]
        }
      ],
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:48.917458Z",
          "iopub.status.busy": "2023-03-08T13:42:48.916942Z",
          "iopub.status.idle": "2023-03-08T13:42:48.920514Z",
          "shell.execute_reply": "2023-03-08T13:42:48.919771Z"
        },
        "id": "C-G2oaTxy6km"
      },
      "outputs": [],
      "source": [
        "seq_length = 100\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZSYAcQV8OGP"
      },
      "source": [
        "The `batch` method lets you easily convert these individual characters to sequences of the desired size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:48.923395Z",
          "iopub.status.busy": "2023-03-08T13:42:48.923106Z",
          "iopub.status.idle": "2023-03-08T13:42:48.941262Z",
          "shell.execute_reply": "2023-03-08T13:42:48.940426Z"
        },
        "id": "BpdjRO2CzOfZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cd08946-35ef-4d1d-e0cf-5eef087794fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'\\n' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' '\n",
            " b' ' b' ' b' ' b' ' b' ' b'Z' b'a' b'r' b'a' b't' b'h' b'u' b's' b't'\n",
            " b'r' b'a' b\"'\" b's' b' ' b'P' b'r' b'o' b'l' b'o' b'g' b'u' b'e' b'\\n'\n",
            " b'\\n' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' '\n",
            " b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' ' b' '\n",
            " b' ' b'1' b'.' b'\\n' b'\\n' b' ' b' ' b'W' b'H' b'E' b'N' b' ' b'Z' b'a'\n",
            " b'r' b'a' b't' b'h' b'u' b's' b't' b'r' b'a' b' ' b'w' b'a' b's' b' '\n",
            " b't' b'h' b'i'], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PHW902-4oZt"
      },
      "source": [
        "It's easier to see what this is doing if you join the tokens back into strings:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:48.944380Z",
          "iopub.status.busy": "2023-03-08T13:42:48.943873Z",
          "iopub.status.idle": "2023-03-08T13:42:48.960429Z",
          "shell.execute_reply": "2023-03-08T13:42:48.959615Z"
        },
        "id": "QO32cMWu4a06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f573f03-1e1f-4329-e959-5eb7d679a5f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b\"\\n                  Zarathustra's Prologue\\n\\n                            1.\\n\\n  WHEN Zarathustra was thi\"\n",
            "b'rty years old, he left his home and the lake\\nof his home, and went into the mountains. There he enjoy'\n",
            "b'ed his\\nspirit and his solitude, and for ten years did not weary of it. But at\\nlast his heart changed,'\n",
            "b'- and rising one morning with the rosy dawn, he\\nwent before the sun, and spake thus unto it:\\n  Thou g'\n",
            "b'reat star! What would be thy happiness if thou hadst not those\\nfor whom thou shinest!\\n  For ten years'\n"
          ]
        }
      ],
      "source": [
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbLcIPBj_mWZ"
      },
      "source": [
        "For training you'll need a dataset of `(input, label)` pairs. Where `input` and\n",
        "`label` are sequences. At each time step the input is the current character and the label is the next character.\n",
        "\n",
        "Here's a function that takes a sequence as input, duplicates, and shifts it to align the input and label for each timestep:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:48.963248Z",
          "iopub.status.busy": "2023-03-08T13:42:48.962971Z",
          "iopub.status.idle": "2023-03-08T13:42:48.966877Z",
          "shell.execute_reply": "2023-03-08T13:42:48.966030Z"
        },
        "id": "9NGu-FkO_kYU"
      },
      "outputs": [],
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:48.969622Z",
          "iopub.status.busy": "2023-03-08T13:42:48.969375Z",
          "iopub.status.idle": "2023-03-08T13:42:48.974625Z",
          "shell.execute_reply": "2023-03-08T13:42:48.973733Z"
        },
        "id": "WxbDTJTw5u_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ef728a5-fc06-4161-8059-8beceb97f087"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:48.977288Z",
          "iopub.status.busy": "2023-03-08T13:42:48.977062Z",
          "iopub.status.idle": "2023-03-08T13:42:49.015627Z",
          "shell.execute_reply": "2023-03-08T13:42:49.014853Z"
        },
        "id": "B9iKPXkw5xwa"
      },
      "outputs": [],
      "source": [
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:49.018438Z",
          "iopub.status.busy": "2023-03-08T13:42:49.018186Z",
          "iopub.status.idle": "2023-03-08T13:42:49.042092Z",
          "shell.execute_reply": "2023-03-08T13:42:49.041437Z"
        },
        "id": "GNbw-iR0ymwj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6f79c3f-e071-4779-b171-1624fe2595fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b\"\\n                  Zarathustra's Prologue\\n\\n                            1.\\n\\n  WHEN Zarathustra was th\"\n",
            "Target: b\"                  Zarathustra's Prologue\\n\\n                            1.\\n\\n  WHEN Zarathustra was thi\"\n"
          ]
        }
      ],
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJdfPmdqzf-R"
      },
      "source": [
        "### Create training batches\n",
        "\n",
        "You used `tf.data` to split the text into manageable sequences. But before feeding this data into the model, you need to shuffle the data and pack it into batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:49.044984Z",
          "iopub.status.busy": "2023-03-08T13:42:49.044498Z",
          "iopub.status.idle": "2023-03-08T13:42:49.054794Z",
          "shell.execute_reply": "2023-03-08T13:42:49.054148Z"
        },
        "id": "p2pGotuNzf-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56a2c49a-a705-4fa5-ce42-309681f2fc32"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "## Build The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:49.057822Z",
          "iopub.status.busy": "2023-03-08T13:42:49.057328Z",
          "iopub.status.idle": "2023-03-08T13:42:49.061288Z",
          "shell.execute_reply": "2023-03-08T13:42:49.060734Z"
        },
        "id": "zHT8cLh7EAsg"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:49.064011Z",
          "iopub.status.busy": "2023-03-08T13:42:49.063450Z",
          "iopub.status.idle": "2023-03-08T13:42:49.068743Z",
          "shell.execute_reply": "2023-03-08T13:42:49.068203Z"
        },
        "id": "wj8HQ2w8z4iO"
      },
      "outputs": [],
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:49.071395Z",
          "iopub.status.busy": "2023-03-08T13:42:49.070879Z",
          "iopub.status.idle": "2023-03-08T13:42:49.085012Z",
          "shell.execute_reply": "2023-03-08T13:42:49.084469Z"
        },
        "id": "IX58Xj9z47Aw"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "## Try the model\n",
        "\n",
        "Now run the model to see that it behaves as expected.\n",
        "\n",
        "First check the shape of the output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:49.087938Z",
          "iopub.status.busy": "2023-03-08T13:42:49.087723Z",
          "iopub.status.idle": "2023-03-08T13:42:51.429257Z",
          "shell.execute_reply": "2023-03-08T13:42:51.428531Z"
        },
        "id": "C-_70kKAPrPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba7a8b21-af7d-4792-c28c-e9346c23a63a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 75) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6NzLBi4VM4o"
      },
      "source": [
        "In the above example the sequence length of the input is `100` but the model can be run on inputs of any length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:51.432685Z",
          "iopub.status.busy": "2023-03-08T13:42:51.432450Z",
          "iopub.status.idle": "2023-03-08T13:42:51.443437Z",
          "shell.execute_reply": "2023-03-08T13:42:51.442808Z"
        },
        "id": "vPGmAAXmVLGC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "646bc138-9edc-4222-f59e-01525334deed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  19200     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  76875     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,034,379\n",
            "Trainable params: 4,034,379\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwv0gEkURfx1"
      },
      "source": [
        "To get actual predictions from the model you need to sample from the output distribution, to get actual character indices. This distribution is defined by the logits over the character vocabulary.\n",
        "\n",
        "Note: It is important to _sample_ from this distribution as taking the _argmax_ of the distribution can easily get the model stuck in a loop.\n",
        "\n",
        "Try it for the first example in the batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:51.446575Z",
          "iopub.status.busy": "2023-03-08T13:42:51.446357Z",
          "iopub.status.idle": "2023-03-08T13:42:51.452729Z",
          "shell.execute_reply": "2023-03-08T13:42:51.452179Z"
        },
        "id": "4V4MfFg0RQJg"
      },
      "outputs": [],
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QM1Vbxs_URw5"
      },
      "source": [
        "This gives us, at each timestep, a prediction of the next character index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:51.455683Z",
          "iopub.status.busy": "2023-03-08T13:42:51.455479Z",
          "iopub.status.idle": "2023-03-08T13:42:51.459587Z",
          "shell.execute_reply": "2023-03-08T13:42:51.459056Z"
        },
        "id": "YqFMUQc_UFgM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31a308b7-edf6-4873-93af-7abe337d3e8e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 4,  7, 63, 15, 70, 35, 63, 56,  6,  1,  4, 73, 64, 17, 58, 20, 30,\n",
              "       65, 54, 67, 64, 73, 20, 34, 26, 50, 32, 47, 65, 35, 42, 13, 42, 54,\n",
              "       25, 67, 63, 70, 13, 65, 66, 57,  2, 46, 17, 43, 31, 29, 16, 74, 26,\n",
              "       67, 18, 39, 70, 12, 18,  7, 10, 59,  4,  6, 51, 66, 40, 32, 37, 74,\n",
              "       21, 67, 47, 41, 54, 17, 27, 34, 33, 49, 22, 62, 71,  2, 63,  5, 38,\n",
              "       39, 57,  1, 52, 40, 66, 32, 74, 73, 52,  1, 42, 67, 50, 41])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "sampled_indices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfLtsP3mUhCG"
      },
      "source": [
        "Decode these to see the text predicted by this untrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:51.462438Z",
          "iopub.status.busy": "2023-03-08T13:42:51.461992Z",
          "iopub.status.idle": "2023-03-08T13:42:51.467740Z",
          "shell.execute_reply": "2023-03-08T13:42:51.467190Z"
        },
        "id": "xWcFwPwLSo05",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a5a04e1-3dab-4eb7-bb7e-d22b2c626d34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'reators and cultivators and sowers of the future;-\\n  -Verily, not to a nobility which ye could purch'\n",
            "\n",
            "Next Char Predictions:\n",
            " b'\")o4vLoh(\\n\"yp6j9Gqfspy9KCbIYqLS2SfBsov2qri W6THF5zCs7Pv17).k\"(crQINz:sYRf6DKJa;nw o\\'OPi\\ndQrIzyd\\nSsbR'\n"
          ]
        }
      ],
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCbHQHiaa4Ic"
      },
      "source": [
        "At this point the problem can be treated as a standard classification problem. Given the previous RNN state, and the input this time step, predict the class of the next character."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trpqTWyvk0nr"
      },
      "source": [
        "### Attach an optimizer, and a loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAjbjY03eiQ4"
      },
      "source": [
        "The standard `tf.keras.losses.sparse_categorical_crossentropy` loss function works in this case because it is applied across the last dimension of the predictions.\n",
        "\n",
        "Because your model returns logits, you need to set the `from_logits` flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:51.470729Z",
          "iopub.status.busy": "2023-03-08T13:42:51.470514Z",
          "iopub.status.idle": "2023-03-08T13:42:51.473573Z",
          "shell.execute_reply": "2023-03-08T13:42:51.472976Z"
        },
        "id": "ZOeWdgxNFDXq"
      },
      "outputs": [],
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:51.476111Z",
          "iopub.status.busy": "2023-03-08T13:42:51.475910Z",
          "iopub.status.idle": "2023-03-08T13:42:51.486964Z",
          "shell.execute_reply": "2023-03-08T13:42:51.486396Z"
        },
        "id": "4HrXTACTdzY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dca59c8-cc3c-4178-ae33-6b6209ea499f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 75)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.315539, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkvUIneTFiow"
      },
      "source": [
        "A newly initialized model shouldn't be too sure of itself, the output logits should all have similar magnitudes. To confirm this you can check that the exponential of the mean loss is approximately equal to the vocabulary size. A much higher loss means the model is sure of its wrong answers, and is badly initialized:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:51.489643Z",
          "iopub.status.busy": "2023-03-08T13:42:51.489428Z",
          "iopub.status.idle": "2023-03-08T13:42:51.494127Z",
          "shell.execute_reply": "2023-03-08T13:42:51.493560Z"
        },
        "id": "MAJfS5YoFiHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f232faad-e6de-4b38-8439-e0ca41b3d86d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "74.85395"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeOXriLcymww"
      },
      "source": [
        "Configure the training procedure using the `tf.keras.Model.compile` method. Use `tf.keras.optimizers.Adam` with default arguments and the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:51.496976Z",
          "iopub.status.busy": "2023-03-08T13:42:51.496456Z",
          "iopub.status.idle": "2023-03-08T13:42:51.508511Z",
          "shell.execute_reply": "2023-03-08T13:42:51.507963Z"
        },
        "id": "DDl1_Een6rL0"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSJdchZggUj"
      },
      "source": [
        "### Configure checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6XBUUavgF56"
      },
      "source": [
        "Use a `tf.keras.callbacks.ModelCheckpoint` to ensure that checkpoints are saved during training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:51.511523Z",
          "iopub.status.busy": "2023-03-08T13:42:51.511306Z",
          "iopub.status.idle": "2023-03-08T13:42:51.514878Z",
          "shell.execute_reply": "2023-03-08T13:42:51.514303Z"
        },
        "id": "W6fWTriUZP-n"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ky3F_BhgkTW"
      },
      "source": [
        "### Execute the training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxdOA-rgyGvs"
      },
      "source": [
        "To keep training time reasonable, use 10 epochs to train the model. In Colab, set the runtime to GPU for faster training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:51.517758Z",
          "iopub.status.busy": "2023-03-08T13:42:51.517554Z",
          "iopub.status.idle": "2023-03-08T13:42:51.520236Z",
          "shell.execute_reply": "2023-03-08T13:42:51.519692Z"
        },
        "id": "7yGBE2zxMMHs"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:42:51.522580Z",
          "iopub.status.busy": "2023-03-08T13:42:51.522377Z",
          "iopub.status.idle": "2023-03-08T13:45:23.259604Z",
          "shell.execute_reply": "2023-03-08T13:45:23.258874Z"
        },
        "id": "UK-hmKjYVoll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ac43ad-b561-4bac-c604-80430c75b1ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "80/80 [==============================] - 18s 74ms/step - loss: 3.0869 - accuracy: 0.2137\n",
            "Epoch 2/20\n",
            "80/80 [==============================] - 6s 69ms/step - loss: 2.3304 - accuracy: 0.3407\n",
            "Epoch 3/20\n",
            "80/80 [==============================] - 6s 53ms/step - loss: 2.0647 - accuracy: 0.4034\n",
            "Epoch 4/20\n",
            "80/80 [==============================] - 5s 55ms/step - loss: 1.8517 - accuracy: 0.4601\n",
            "Epoch 5/20\n",
            "80/80 [==============================] - 5s 53ms/step - loss: 1.6922 - accuracy: 0.5019\n",
            "Epoch 6/20\n",
            "80/80 [==============================] - 5s 54ms/step - loss: 1.5749 - accuracy: 0.5318\n",
            "Epoch 7/20\n",
            "80/80 [==============================] - 5s 56ms/step - loss: 1.4856 - accuracy: 0.5542\n",
            "Epoch 8/20\n",
            "80/80 [==============================] - 6s 55ms/step - loss: 1.4153 - accuracy: 0.5728\n",
            "Epoch 9/20\n",
            "80/80 [==============================] - 5s 55ms/step - loss: 1.3538 - accuracy: 0.5887\n",
            "Epoch 10/20\n",
            "80/80 [==============================] - 5s 56ms/step - loss: 1.3017 - accuracy: 0.6020\n",
            "Epoch 11/20\n",
            "80/80 [==============================] - 5s 56ms/step - loss: 1.2520 - accuracy: 0.6152\n",
            "Epoch 12/20\n",
            "80/80 [==============================] - 6s 60ms/step - loss: 1.2053 - accuracy: 0.6277\n",
            "Epoch 13/20\n",
            "80/80 [==============================] - 6s 57ms/step - loss: 1.1575 - accuracy: 0.6402\n",
            "Epoch 14/20\n",
            "80/80 [==============================] - 5s 57ms/step - loss: 1.1104 - accuracy: 0.6536\n",
            "Epoch 15/20\n",
            "80/80 [==============================] - 6s 58ms/step - loss: 1.0625 - accuracy: 0.6675\n",
            "Epoch 16/20\n",
            "80/80 [==============================] - 6s 59ms/step - loss: 1.0109 - accuracy: 0.6828\n",
            "Epoch 17/20\n",
            "80/80 [==============================] - 6s 59ms/step - loss: 0.9556 - accuracy: 0.6994\n",
            "Epoch 18/20\n",
            "80/80 [==============================] - 6s 60ms/step - loss: 0.9000 - accuracy: 0.7163\n",
            "Epoch 19/20\n",
            "80/80 [==============================] - 6s 61ms/step - loss: 0.8403 - accuracy: 0.7360\n",
            "Epoch 20/20\n",
            "80/80 [==============================] - 6s 62ms/step - loss: 0.7767 - accuracy: 0.7568\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjGz1tDkzf-u"
      },
      "source": [
        "The following makes a single step prediction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:45:23.263344Z",
          "iopub.status.busy": "2023-03-08T13:45:23.263066Z",
          "iopub.status.idle": "2023-03-08T13:45:23.271251Z",
          "shell.execute_reply": "2023-03-08T13:45:23.270613Z"
        },
        "id": "iSBU1tHmlUSs"
      },
      "outputs": [],
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:45:23.273912Z",
          "iopub.status.busy": "2023-03-08T13:45:23.273471Z",
          "iopub.status.idle": "2023-03-08T13:45:23.287374Z",
          "shell.execute_reply": "2023-03-08T13:45:23.286753Z"
        },
        "id": "fqMOuDutnOxK"
      },
      "outputs": [],
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9yDoa0G3IgQ"
      },
      "source": [
        "Run it in a loop to generate some text. Looking at the generated text, you'll see the model knows when to capitalize, make paragraphs and imitates a Shakespeare-like writing vocabulary. With the small number of training epochs, it has not yet learned to form coherent sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:45:23.290500Z",
          "iopub.status.busy": "2023-03-08T13:45:23.289873Z",
          "iopub.status.idle": "2023-03-08T13:45:25.845517Z",
          "shell.execute_reply": "2023-03-08T13:45:25.844738Z"
        },
        "id": "ST7PSyk9t1mT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7157a0fd-5b66-41a3-b5b0-0f5523376680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stars, and would fain\n",
            "came, as from failures, laughing at\n",
            "this gloomy throughous such a God, like the\n",
            "scrobible, or affliction, and said:\n",
            "  The shore the ten truths and exable himself.\n",
            "  \"But whatever happiness, unwilling to love and exulted:\n",
            "  Everything injuredy and wickedest that he died too\n",
            "young; one thing is mixnound themselves, like all\n",
            "Uple?\n",
            "  Ever virtue and a little cold, and his\n",
            "seat for itself good and evil:- how wilt thou\n",
            "remain all the flock of nothing service! So that\n",
            "the night and the strangest thing, a physician: thy soul with leafter'ly about it up my clambera; verily, with worthy about, let\n",
            "\n",
            "  ThE forest have I found the mourtain- oppresseth with great mouth. Ready to the\n",
            "Name wishest not yet come,- what testity on more of my heart?\n",
            "Well then, thou creath for thyself? As yet it still the country of my cave: thouragn hast thou\n",
            "animals, thas good things; already not of ununglock. The dead, me for one and\n",
            "there! Hust!\n",
            "They call thee uptared of us power: so have perpease it a \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.45228910446167\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['stars'])\n",
        "result = [next_char]\n",
        "\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_OfbI4aULmuj"
      },
      "source": [
        "If you want the model to generate text *faster* the easiest thing you can do is batch the text generation. In the example below the model generates 5 outputs in about the same time it took to generate 1 above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:45:25.849289Z",
          "iopub.status.busy": "2023-03-08T13:45:25.848579Z",
          "iopub.status.idle": "2023-03-08T13:45:28.243340Z",
          "shell.execute_reply": "2023-03-08T13:45:28.242690Z"
        },
        "id": "ZkLu7Y8UCMT7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9116f2d-9dcd-46ea-c677-cf7dc1a0038c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'stars shall no one talk\\nthus creative enamourity, those gods of springing leg,\\nand their drunken, cannot midnight, and leisure it, many think imposed upon;\\n  -In thee! Sumperied over day: so maketh them had not seek. Do not tender youn away!\\n  And we went a frighten at his head wno lost them to the tarantly\\nones! Since he was forsakenness pretence.\\n  Gladly, thou thyself here as hath hanneth. To the incline in his mountain to me.\\n  \"We have loved his foremoth as with soor poverty.\\n  Go out unto thee,'\n",
            " b'stars, however, whose\\nmarriage-runger!\\n  Smitten and jaughing, wings, thought out upon drien than\\nany\\nmore. Must now have your happiness shall he new\\ntruth.- all grey laughter, place imperfect cripde;\\nand this man idmost a\\nbitter unto me, thou shright!-\\n  -Then, ye might seek and become awar, that the\\nlightest abso betimes, and wanteth to our fover round the\\nhumanlest back; and, say slay all at once\\ntheir limps of my type have the geturn and\\nawfroyed, tooken yeil openly: For thus\\nseeketh the stormitu'\n",
            " b'stars there is like them as ith\\nreverent! Radness, however, had he his feet, his ashamed\\nof choits. For these are no one\\nto say and suffereth from the bad conscience.\"-\\n  When Zarathustra, however, spake these words, when they know hearty, and withdraldge to give unators of and\\nforeterningly: \"Well! The spirit of gravity- Platened mountains, an equel where ou\\nsay unto\\nyou? He who maketh hath hast loose- year thee, my\\nchilbrbous, I forefort, ye can no one!\\n  For thish Zarathustra who cleareth turn upo'\n",
            " b'stars war to\\nthe earth! But the stripe flown alof- it for them that in\\ntheir old manslaughter hidwint, then doth it risont\\nbellow open auroundeth.\\n  At last, so higher sensit ascend, and else how\\nto whom distanteth to drink himself, he\\nheard these words: \"The market-placed but\\nlive, and a goather, and what is there told him and still sleeph\\nmasteringly of soul. But something be the hardest thing in\\nthe stillest miding than all rounded old man;\\na gness more there is no thank corn; but he\\ntell me: othe'\n",
            " b'stars, still from him\\ndroughty of man is a child, and suppressed mountains.\\n\"but am I for for me to the people of the marriage.-\\n  Higher doth he look at these discourse? What master over above all reaconscious man.\"\\n  \"Pity thereficence is always the stars thinkward choke; which peastituing poison-worldly all the wild distrust, and whoever given thee their crying; they\\nsaid a dog that I may my storm. Then, when they call them all\\nthy shame: so that the eagle, however, luben eyes,\\ndid it not be ye do'], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 1.2937541007995605\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['stars', 'stars', 'stars', 'stars', 'stars'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(500):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlUQzwu6EXam"
      },
      "source": [
        "## Export the generator\n",
        "\n",
        "This single-step model can easily be [saved and restored](https://www.tensorflow.org/guide/saved_model), allowing you to use it anywhere a `tf.saved_model` is accepted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:45:28.246316Z",
          "iopub.status.busy": "2023-03-08T13:45:28.246076Z",
          "iopub.status.idle": "2023-03-08T13:45:33.985714Z",
          "shell.execute_reply": "2023-03-08T13:45:33.984828Z"
        },
        "id": "3Grk32H_CzsC",
        "outputId": "6ec13106-bb91-441f-c560-c542481a3b0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7975b81a29b0>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
          ]
        }
      ],
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2023-03-08T13:45:33.989565Z",
          "iopub.status.busy": "2023-03-08T13:45:33.989010Z",
          "iopub.status.idle": "2023-03-08T13:45:34.429550Z",
          "shell.execute_reply": "2023-03-08T13:45:34.428824Z"
        },
        "id": "_Z9bb_wX6Uuu",
        "outputId": "b4a9deb0-fc98-4daf-b018-7d4479b7414d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "love and of all egrained happy, all good and bad lords\n",
            "in all new thoughts, and the fuller then only\n",
            "do \n"
          ]
        }
      ],
      "source": [
        "states = None\n",
        "next_char = tf.constant(['love'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pzMrYD-32QND"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}